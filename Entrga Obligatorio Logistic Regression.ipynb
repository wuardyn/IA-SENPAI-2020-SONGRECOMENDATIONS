{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Entrga Obligatorio Logistic Regression.ipynb","provenance":[{"file_id":"10zP5O1ybOm90B9kyItzoTJfy5DiBMRez","timestamp":1607891051816}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"-7hK21wydLEF"},"source":["# Cargo el CSV de reproducciones eventos (Reproducciones y \"No Reproducciones\")"]},{"cell_type":"code","metadata":{"id":"X9WL7q0hv08M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608000964210,"user_tz":180,"elapsed":721,"user":{"displayName":"Manuel Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvTAjM5P6tFyrk760hpFBr7zEbEEUoGAAdqb6IEw=s64","userId":"01164239875507823605"}},"outputId":"faf99df2-9f76-432f-833b-216d956fc69a"},"source":["# Conecta con gdrive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ucnmQICRda2r"},"source":["# Importa el CSV\n","import pandas as pd, numpy as np\n","dataframe = pd.read_csv('drive/MyDrive/AI/entrega_obligatorio/eventos_20201201_20201203_mas_10_noreps.csv')\n","# Elimina filas con columnas con valores Nan \n","for column in list(dataframe.columns):\n","  nan_rows = dataframe[dataframe[column].isnull()]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QF-IYDZ69QIJ","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1608000969591,"user_tz":180,"elapsed":6072,"user":{"displayName":"Manuel Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvTAjM5P6tFyrk760hpFBr7zEbEEUoGAAdqb6IEw=s64","userId":"01164239875507823605"}},"outputId":"cbad595c-2471-4863-879b-14c22650c684"},"source":["# Muestra Dataframe\n","dataframe"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>usuario</th>\n","      <th>id_pais</th>\n","      <th>id_aplicacion</th>\n","      <th>id_track</th>\n","      <th>id_album</th>\n","      <th>id_artista</th>\n","      <th>id_playlist</th>\n","      <th>id_genero</th>\n","      <th>fecha_lanzamiento</th>\n","      <th>reproduccion</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>12462561083</td>\n","      <td>52</td>\n","      <td>6</td>\n","      <td>101269</td>\n","      <td>9569</td>\n","      <td>949</td>\n","      <td>51322</td>\n","      <td>32</td>\n","      <td>1230768000</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>12462561083</td>\n","      <td>52</td>\n","      <td>6</td>\n","      <td>10172245</td>\n","      <td>1876931</td>\n","      <td>1043198</td>\n","      <td>51322</td>\n","      <td>4</td>\n","      <td>1555632000</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>12462561083</td>\n","      <td>52</td>\n","      <td>6</td>\n","      <td>10260957</td>\n","      <td>1878464</td>\n","      <td>838972</td>\n","      <td>33022</td>\n","      <td>1041</td>\n","      <td>1555286400</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>12462561083</td>\n","      <td>52</td>\n","      <td>6</td>\n","      <td>10478308</td>\n","      <td>1931685</td>\n","      <td>1076154</td>\n","      <td>51322</td>\n","      <td>929</td>\n","      <td>1557446400</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>12462561083</td>\n","      <td>52</td>\n","      <td>6</td>\n","      <td>10573965</td>\n","      <td>1948449</td>\n","      <td>1086372</td>\n","      <td>14117</td>\n","      <td>868</td>\n","      <td>1558224000</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5513957</th>\n","      <td>59996980044</td>\n","      <td>531</td>\n","      <td>6</td>\n","      <td>33439640</td>\n","      <td>6370070</td>\n","      <td>120066</td>\n","      <td>91959</td>\n","      <td>21</td>\n","      <td>1606694400</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5513958</th>\n","      <td>18763095386</td>\n","      <td>388</td>\n","      <td>6</td>\n","      <td>33444360</td>\n","      <td>6371088</td>\n","      <td>379828</td>\n","      <td>-1</td>\n","      <td>21</td>\n","      <td>1606780800</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5513959</th>\n","      <td>18768629026</td>\n","      <td>388</td>\n","      <td>6</td>\n","      <td>33444360</td>\n","      <td>6371088</td>\n","      <td>379828</td>\n","      <td>-1</td>\n","      <td>21</td>\n","      <td>1606780800</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5513960</th>\n","      <td>50373702509</td>\n","      <td>222</td>\n","      <td>6</td>\n","      <td>33453744</td>\n","      <td>6372303</td>\n","      <td>363687</td>\n","      <td>-1</td>\n","      <td>4</td>\n","      <td>1606780800</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5513961</th>\n","      <td>50944406456</td>\n","      <td>332</td>\n","      <td>6</td>\n","      <td>33478149</td>\n","      <td>6376338</td>\n","      <td>3079003</td>\n","      <td>-1</td>\n","      <td>1674</td>\n","      <td>1606780800</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5513962 rows × 10 columns</p>\n","</div>"],"text/plain":["             usuario  id_pais  ...  fecha_lanzamiento  reproduccion\n","0        12462561083       52  ...         1230768000             0\n","1        12462561083       52  ...         1555632000             0\n","2        12462561083       52  ...         1555286400             0\n","3        12462561083       52  ...         1557446400             0\n","4        12462561083       52  ...         1558224000             0\n","...              ...      ...  ...                ...           ...\n","5513957  59996980044      531  ...         1606694400             1\n","5513958  18763095386      388  ...         1606780800             1\n","5513959  18768629026      388  ...         1606780800             1\n","5513960  50373702509      222  ...         1606780800             1\n","5513961  50944406456      332  ...         1606780800             1\n","\n","[5513962 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"PbTrU5GFtRqn"},"source":["#Normalización\n","El dataset contiene columnas con identificadores, donde alguna de ellas tienen números muy altos (varios millones) , por este motivo es necesario la normalización de las columnas, para evitar la sobrerepresentación de alguna de ellas.\n","\n","Se guardan parámetros de normalización (variable meanStd) para utilizar posteriormente al invocar el modelo.\n"]},{"cell_type":"code","metadata":{"id":"HbKqnmD8eb_g"},"source":["# Castea a double y normalizo todas las columnas\n","columns = list(dataframe.columns)\n","meanStd = list()\n"," \n","NormDataframe = dataframe\n"," \n","for column in columns:\n","    mean =  dataframe[column].astype(np.double).mean()\n","    std = dataframe[column].astype(np.double).std()\n","    max = dataframe[column].astype(np.double).max()\n","    \n","    if column == 'fecha_lanzamiento':\n","      if len(dataframe[dataframe[column].isnull()]) == 0:\n","        NormDataframe[column] = (mean / std)\n","      else:\n","        NormDataframe[column] = (dataframe[column].astype(np.double) - mean) / std\n","      meanStd.append([-1, -1, -1])\n","    elif column == 'reproduccion': \n","      1==1\n","    else:\n","      meanStd.append([mean, std, -1])\n","      NormDataframe[column] = (dataframe[column].astype(np.double) - mean) / std"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GRzr64STvahr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608000971927,"user_tz":180,"elapsed":8389,"user":{"displayName":"Manuel Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvTAjM5P6tFyrk760hpFBr7zEbEEUoGAAdqb6IEw=s64","userId":"01164239875507823605"}},"outputId":"e29a0f2f-a295-4b27-9561-a55941a73f86"},"source":["# Muestra los datos de promedio y desviacion standard de cada columna (variable meanStd)\n","meanStd"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[206908860914.6384, 251587825784.14664, -1],\n"," [348.20748510780453, 210.2791382386964, -1],\n"," [6.011691049013395, 2.2632218262099517, -1],\n"," [11670313.054696424, 10838318.443102755, -1],\n"," [2229712.8013169114, 2121160.5107313045, -1],\n"," [432378.42190751404, 697693.858503115, -1],\n"," [30650.722141538154, 44067.83528885183, -1],\n"," [369.7700869175377, 491.60563500878794, -1],\n"," [-1, -1, -1]]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"fL41_NK_hLsp","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1608000971928,"user_tz":180,"elapsed":8380,"user":{"displayName":"Manuel Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvTAjM5P6tFyrk760hpFBr7zEbEEUoGAAdqb6IEw=s64","userId":"01164239875507823605"}},"outputId":"63738e2b-e8c5-4618-cd89-57bf4a987a30"},"source":["# Muestra Dataframe Normalizado\n","NormDataframe"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>usuario</th>\n","      <th>id_pais</th>\n","      <th>id_aplicacion</th>\n","      <th>id_track</th>\n","      <th>id_album</th>\n","      <th>id_artista</th>\n","      <th>id_playlist</th>\n","      <th>id_genero</th>\n","      <th>fecha_lanzamiento</th>\n","      <th>reproduccion</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.772876</td>\n","      <td>-1.408639</td>\n","      <td>-0.005166</td>\n","      <td>-1.067421</td>\n","      <td>-1.046665</td>\n","      <td>-0.618365</td>\n","      <td>0.469079</td>\n","      <td>-0.687075</td>\n","      <td>3.058823</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.772876</td>\n","      <td>-1.408639</td>\n","      <td>-0.005166</td>\n","      <td>-0.138220</td>\n","      <td>-0.166315</td>\n","      <td>0.875484</td>\n","      <td>0.469079</td>\n","      <td>-0.744032</td>\n","      <td>3.058823</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.772876</td>\n","      <td>-1.408639</td>\n","      <td>-0.005166</td>\n","      <td>-0.130035</td>\n","      <td>-0.165593</td>\n","      <td>0.582768</td>\n","      <td>0.053810</td>\n","      <td>1.365383</td>\n","      <td>3.058823</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-0.772876</td>\n","      <td>-1.408639</td>\n","      <td>-0.005166</td>\n","      <td>-0.109981</td>\n","      <td>-0.140502</td>\n","      <td>0.922719</td>\n","      <td>0.469079</td>\n","      <td>1.137558</td>\n","      <td>3.058823</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.772876</td>\n","      <td>-1.408639</td>\n","      <td>-0.005166</td>\n","      <td>-0.101155</td>\n","      <td>-0.132599</td>\n","      <td>0.937365</td>\n","      <td>-0.375188</td>\n","      <td>1.013475</td>\n","      <td>3.058823</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5513957</th>\n","      <td>-0.583939</td>\n","      <td>0.869285</td>\n","      <td>-0.005166</td>\n","      <td>2.008552</td>\n","      <td>1.951930</td>\n","      <td>-0.447635</td>\n","      <td>1.391225</td>\n","      <td>-0.709451</td>\n","      <td>3.058823</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5513958</th>\n","      <td>-0.747833</td>\n","      <td>0.189237</td>\n","      <td>-0.005166</td>\n","      <td>2.008988</td>\n","      <td>1.952410</td>\n","      <td>-0.075320</td>\n","      <td>-0.695558</td>\n","      <td>-0.709451</td>\n","      <td>3.058823</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5513959</th>\n","      <td>-0.747811</td>\n","      <td>0.189237</td>\n","      <td>-0.005166</td>\n","      <td>2.008988</td>\n","      <td>1.952410</td>\n","      <td>-0.075320</td>\n","      <td>-0.695558</td>\n","      <td>-0.709451</td>\n","      <td>3.058823</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5513960</th>\n","      <td>-0.622189</td>\n","      <td>-0.600190</td>\n","      <td>-0.005166</td>\n","      <td>2.009853</td>\n","      <td>1.952983</td>\n","      <td>-0.098455</td>\n","      <td>-0.695558</td>\n","      <td>-0.744032</td>\n","      <td>3.058823</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5513961</th>\n","      <td>-0.619921</td>\n","      <td>-0.077076</td>\n","      <td>-0.005166</td>\n","      <td>2.012105</td>\n","      <td>1.954885</td>\n","      <td>3.793390</td>\n","      <td>-0.695558</td>\n","      <td>2.653000</td>\n","      <td>3.058823</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5513962 rows × 10 columns</p>\n","</div>"],"text/plain":["          usuario   id_pais  ...  fecha_lanzamiento  reproduccion\n","0       -0.772876 -1.408639  ...           3.058823             0\n","1       -0.772876 -1.408639  ...           3.058823             0\n","2       -0.772876 -1.408639  ...           3.058823             0\n","3       -0.772876 -1.408639  ...           3.058823             0\n","4       -0.772876 -1.408639  ...           3.058823             0\n","...           ...       ...  ...                ...           ...\n","5513957 -0.583939  0.869285  ...           3.058823             1\n","5513958 -0.747833  0.189237  ...           3.058823             1\n","5513959 -0.747811  0.189237  ...           3.058823             1\n","5513960 -0.622189 -0.600190  ...           3.058823             1\n","5513961 -0.619921 -0.077076  ...           3.058823             1\n","\n","[5513962 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"aD_thRuI2eJF"},"source":["#Separamos el dataset en conjunto de test y de validación"]},{"cell_type":"code","metadata":{"id":"RytGeD5hzKiS"},"source":["# Porcentaje de datos para training.\n","p_train = 0.70 \n","\n","# Define las columnas que seran de para training y validacion agregando una columna\n","NormDataframe = NormDataframe.sample(frac=1)\n","NormDataframe['is_train'] = np.random.uniform(0, 1, len(NormDataframe)) <= p_train\n","trainX, testX = NormDataframe[NormDataframe['is_train']==True], NormDataframe[NormDataframe['is_train']==False]\n","\n","# Extrae de ambos dataframes los valores de evaluacion de la columna reproduccion\n","y_train_dataframe = trainX['reproduccion']\n","y_test_dataframe = testX['reproduccion']\n","\n","# Elimina de ambos dataframes la columna reproduccion\n","trainX = trainX.drop(['reproduccion'], axis=1)\n","testX = testX.drop(['reproduccion'], axis=1)\n","\n","# Elimina de ambos dataframes la columna is_train utilizada para la partición del dataset\n","testX = testX.drop(['is_train'], axis=1)\n","trainX = trainX.drop(['is_train'], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mdgCBktnrMhP"},"source":["# Convierte los dataframes de test y validacion a Numpy Arrays\n","X = trainX.to_numpy()\n","X_valid = testX.to_numpy()\n","\n","# Convierte a Numpy Arrays y transpone los vectores de resultados\n","y_aux = y_train_dataframe.to_numpy();\n","y = np.array( list(map(lambda x: 0 if x else 1, y_aux)) )[np.newaxis]\n","y = y.T\n","y_valid_aux = y_test_dataframe.to_numpy()\n","y_valid = np.array( list(map(lambda x: 0 if x else 1, y_valid_aux)) )[np.newaxis]\n","y_valid = y_valid.T"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t25GFXMsoALx"},"source":["# Genera los Dataloaders\n","Crea Dataloaders de Pytorch que facilitan el procesamiento en batch y proveen de segmentos de datos \"shufleados\" para evitar sesgos en el ajuste de parámetros"]},{"cell_type":"code","metadata":{"id":"ynpZJNnvoEBW"},"source":["import torch\n","import torch.optim as optim # Contains optimizers such as SGD, which update the weights of Parameter during the backward step\n","import torch.nn as nn\n","import torch.nn.functional as F # contains activation functions, loss functions, etc, as well as non-stateful versions of layers such as convolutional and linear layers\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","# Convierte los Numpy Arrays a Tensores de Pytorch de tipo DoubleTensor\n","X, y, X_valid, y_valid = map(torch.DoubleTensor, (X, y, X_valid, y_valid))\n","\n","# Crea los Dataloaders de entrenamiento y validacion\n","train_ds = TensorDataset(X, y)\n","train_dl = DataLoader(train_ds, batch_size=128, shuffle=True)\n","valid_ds = TensorDataset(X_valid, y_valid)\n","valid_dl = DataLoader(valid_ds, batch_size=256, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DcwEBP3VrZjC"},"source":["#Funciones para calculo de Metricas"]},{"cell_type":"code","metadata":{"id":"TbCOCo57U_2_"},"source":["from sklearn.metrics import precision_score\n","from sklearn.metrics import f1_score\n","import pandas as pd, numpy as np\n","\n","# Define funciones con Metricas para evaluación durante ejecución\n","def metrics(umbral, pred):\n","\n","  numpy_valid = y_valid.detach().numpy()\n","  numpy_pred = pred.detach().numpy()\n","  numpy_pred=numpy_pred[:,-1]\n","  \n","  cant_valid = 0\n","  cant_pred = 0\n","  falso_pos = 0\n","  for a, b in zip(numpy_valid, numpy_pred):    \n","    if a == 1:\n","      cant_valid = cant_valid+1\n","    if b >= umbral:\n","      cant_pred = cant_pred+1\n","    if a == 0 and b >= umbral:\n","      falso_pos = falso_pos+1\n","\n","  numpy_pred_bin = np.array( list(map(lambda x: 0 if x < umbral else 1, numpy_pred)) )\n","  print(f\"real {cant_valid} pred: {cant_pred} fp: {falso_pos} tf: {cant_pred-falso_pos} precision {precision_score(numpy_valid, numpy_pred_bin, average='macro')}\")\n","\n","def metrics2(umbral, pred, y_valid):\n","  numpy_valid = y_valid.detach().numpy()\n","  numpy_pred = pred.detach().numpy()  \n","  cant_valid = 0\n","  cant_negativo = 0\n","  cant_predT = 0\n","  cant_predF = 0\n","  falso_pos = 0\n","  falso_negativo = 0\n","  true_negativo = 0\n","  for a, b in zip(numpy_valid, numpy_pred):    \n","    if a == 1:\n","      cant_valid += 1\n","    else:\n","      cant_negativo += 1\n","   \n","    if b >= umbral:\n","      cant_predT += 1      \n","      if a == 0:\n","        falso_pos += 1     \n","    else:\n","      cant_predF += 1          \n","      if a==0:\n","        true_negativo += 1     \n","      if a==1:\n","        falso_negativo += 1             \n","  \n","  numpy_pred_bin = np.array( list(map(lambda x: 0 if x < umbral else 1, numpy_pred)) )\n","  validPredT =(cant_predT-falso_pos)* 100 / (cant_valid)\n","  validPredF =(cant_predF-falso_negativo)* 100 / (cant_negativo)\n","  print(f\"umbral:{umbral} pos:{cant_valid}  pred:{cant_predT} fp:{falso_pos} fn:{falso_negativo} tp:{cant_predT-falso_pos} neg:{cant_negativo} tn:{true_negativo} precision:{precision_score(numpy_valid, numpy_pred_bin, average='macro')} F1 :{f1_score(numpy_valid, numpy_pred_bin)} predicciones acertadas POS:{validPredT} % NEG:{validPredF} %\")\n","  #sklearn.metrics.f1_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eegcirbg4LKt"},"source":["# Guarda la hora\r\n","import time;\r\n","ts = time.time()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PNAZ_QkXU_x5","executionInfo":{"status":"ok","timestamp":1608000975700,"user_tz":180,"elapsed":12094,"user":{"displayName":"Manuel Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvTAjM5P6tFyrk760hpFBr7zEbEEUoGAAdqb6IEw=s64","userId":"01164239875507823605"}},"outputId":"ca182ca2-4556-4ee5-90db-99f86dc37845"},"source":["# Chequea que este habilitado el entrenamiento por GPU\n","print(torch.cuda.is_available())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-Y2A_2DX-j0n"},"source":["# Define el Modelo"]},{"cell_type":"code","metadata":{"id":"Bzdvo0xmrf1q"},"source":["from datetime import datetime\n"," \n","date_time_running = datetime.now().strftime(\"%m%d%Y%H%M%S\")\n"," \n","# Define tamaño del input y maximo de neuronas en las capas centrales\n","input_size = X.shape[1]\n","hidden_size = 72\n"," \n","# Define el modelo\n","model = nn.Sequential(\n","    \n","          nn.Linear(input_size, int(hidden_size/8)),\n","          nn.LeakyReLU(),\n","          nn.Linear(int(hidden_size/8), int(hidden_size/4)),\n","          nn.LeakyReLU(),\n","          nn.Linear(int(hidden_size/4), int(hidden_size/2)),\n","          nn.LeakyReLU(),\n","          nn.Linear(int(hidden_size/2), hidden_size),\n","          nn.LeakyReLU(),\n","          nn.Linear(hidden_size, hidden_size),\n","          nn.LeakyReLU(),\n","          nn.Linear(hidden_size, hidden_size),\n","          nn.LeakyReLU(),    \n","          nn.Linear(hidden_size, hidden_size),\n","          nn.LeakyReLU(), \n","          nn.Linear(hidden_size, hidden_size),\n","          nn.LeakyReLU(),\n","          nn.Linear(hidden_size, hidden_size),\n","          nn.LeakyReLU(),\n","          nn.Linear(hidden_size, hidden_size),\n","          nn.LeakyReLU(),\n","          nn.Linear(hidden_size, int(hidden_size/2)),\n","          nn.LeakyReLU(),\n","          nn.Linear(int(hidden_size/2), int(hidden_size/4)),\n","          nn.LeakyReLU(),\n","          nn.Linear(int(hidden_size/4), int(hidden_size/8)),\n","          nn.LeakyReLU(),\n","          nn.Linear(int(hidden_size/8), 1),\n","          nn.Sigmoid()\n","        )\n"," \n","# Castea los tensores a FloatTensor\n","X = X.type(torch.FloatTensor)\n","X_valid = X_valid.type(torch.FloatTensor)\n"," \n","y = y.type(torch.FloatTensor)\n","y_valid = y_valid.type(torch.FloatTensor)\n"," \n","# Crea los DataLoaders\n","train_ds = TensorDataset(X, y)\n","train_dl = DataLoader(train_ds, batch_size=256, shuffle=True)\n"," \n","valid_ds = TensorDataset(X_valid, y_valid)\n","valid_dl = DataLoader(valid_ds, batch_size=256, shuffle=True)\n"," \n","# Define Epochs y Learning Rate\n","alpha = 0.001\n","epochs = 200\n","\n","#loss = nn.BCELoss()\n","loss = nn.MSELoss()\n","\n","opt = optim.Adam(model.parameters(), lr=alpha)\n"," \n"," \n","torch.save(train_dl, f\"drive/MyDrive/AI/entrega_obligatorio/modelos/train_dl_{date_time_running}.DS\")\n","torch.save(valid_dl, f\"drive/MyDrive/AI/entrega_obligatorio/modelos/valid_dl_{date_time_running}.DS\")\n","torch.save(meanStd, f\"drive/MyDrive/AI/entrega_obligatorio/modelos/meanStd_{date_time_running}.DS\")\n","\n","\n","dif = 0\n","minLoss= 10000\n","epocMinLoss = 0\n","for epoch in range(epochs):\n","\n","    if epoch%5 == 1:\n","      for g in opt.param_groups:\n","        g['lr'] = g['lr']*0.9\n","\n","    model.train()\n","    for Xb, yb in train_dl:\n","        # Fwd\n","        pred = model(Xb)\n","        output = loss(pred, yb)\n","        # Bwd\n","        output.backward()\n","        # Update params\n","        opt.step()\n","        opt.zero_grad()\n"," \n","    model.eval()\n","    with torch.no_grad():\n","        valid_loss = sum(loss(model(Xb), yb) for Xb, yb in valid_dl)\n","        \n","        if valid_loss < minLoss:\n","          minLoss=valid_loss\n","          epocMinLoss=epoch\n","          torch.save(model, f\"drive/MyDrive/AI/entrega_obligatorio/modelos/model-MSELoss-Adam-alfa-{alpha}-{date_time_running}-{minLoss}.mod\")\n","          #torch.save(model, f\"drive/MyDrive/AI/modelos/model-MSELoss-Adam-alfa-{alpha}-{date_time_running}-{minLoss}.mod\")\n","          pred = model(X_valid)          \n","          metrics2(0.5, pred, y_valid)\n","        print(f\"epoc: {epoch} loss {valid_loss} dif paso anterior {valid_loss-dif} minLoss {minLoss} epocMinLoss {epocMinLoss} time {datetime.now()} \" )\n","        dif = valid_loss\n","        if 10 > dif:\n","            break\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G-QOXG-Y1Liy"},"source":["#Métricas"]},{"cell_type":"code","metadata":{"id":"xAU8sOeX1Khs"},"source":["X_valid= valid_dl.dataset.tensors[0]\n","y_valid= valid_dl.dataset.tensors[1]\n"," \n","pred = model(X_valid)\n","print(pred.shape)\n"," \n","metrics2(0.5, pred, y_valid)\n","metrics2(0.55, pred, y_valid)\n","metrics2(0.6, pred, y_valid)\n","metrics2(0.65, pred, y_valid)\n","metrics2(0.7, pred, y_valid)\n","metrics2(0.75, pred, y_valid)\n","metrics2(0.8, pred, y_valid)\n","metrics2(0.85, pred, y_valid)\n","metrics2(0.9, pred, y_valid)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N2KhHbbA-AVv"},"source":["Evaluación de caso "]},{"cell_type":"code","metadata":{"id":"uG2Yerkl4HsO"},"source":["def normalize(dataframe, meanStd):\r\n","  NormDataframe = dataframe\r\n","  columns = list(dataframe.columns)\r\n","\r\n","  for column, m in zip(columns, meanStd):\r\n","    print(column)\r\n","    if column == 'fecha_reproduccion':\r\n","      NormDataframe[column] = dataframe[column].astype(np.double) / m[2]\r\n","    elif column == 'reproduccion': \r\n","      1==1\r\n","    else:\r\n","      NormDataframe[column] = (dataframe[column].astype(np.double) - m[0]) / m[1]\r\n","   \r\n","  return NormDataframe\r\n","\r\n","\r\n","import pandas as pd, numpy as np\r\n","dataframe_part = pd.read_csv('drive/MyDrive/AI/evento_particular.csv')\r\n","dataframe_part = normalize(dataframe_part, meanStd)\r\n","\r\n","dataframe_part = dataframe_part.drop(['reproduccion'], axis=1)\r\n","\r\n","X_part = torch.from_numpy(dataframe_part.to_numpy())\r\n","X_part = X.type(torch.FloatTensor)\r\n","#print(X_part)\r\n","\r\n","pred = model(X_part)\r\n","\r\n","pred.item()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bVH-6Qtx4LzL"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"twBwhVCejuXH"},"source":["from sklearn.metrics import precision_score\n","\n","#torch.save(model,'drive/MyDrive/AI/model50relusgd72020.mod')\n","cant_valid = 0\n","cant_pred = 0\n","falso_pos = 0\n","\n","umbral = 0.7\n","pred = model(X_valid)\n","\n","numpy_valid = y_valid.detach().numpy()\n","numpy_pred = pred.detach().numpy()\n","numpy_pred=numpy_pred[:,-1]\n","\n","\n","#print(numpy_pred[:,-1])\n","for a, b in zip(numpy_valid, numpy_pred):    \n","  if a == 1:\n","    cant_valid = cant_valid+1\n","  if b >= umbral:\n","    cant_pred = cant_pred+1\n","  if a == 0 and b >= umbral:\n","    falso_pos = falso_pos+1\n","\n","print(cant_valid)\n","print(cant_pred)\n","print(falso_pos)\n","print(cant_pred-falso_pos)\n","\n","numpy_pred_bin = np.array( list(map(lambda x: 0 if x < umbral else 1, numpy_pred)) )\n","res = precision_score(numpy_valid, numpy_pred_bin, average='macro')\n","print(res)\n","\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5pEgUefEJYW-"},"source":["# Ejecuto Pruebas Particulares"]},{"cell_type":"code","metadata":{"id":"YCmfzEv3Ia4r"},"source":["\n","def normalize(dataframe, meanStd):\n","  NormDataframe = dataframe\n","  columns = list(dataframe.columns)\n","\n","  for column, m in zip(columns, meanStd):\n","    print(column)\n","    if column == 'dia_semana':\n","      NormDataframe[column] = dataframe[column].astype(np.double) / 7\n","    elif column == 'fecha_reproduccion':\n","      NormDataframe[column] = dataframe[column].astype(np.double) / m[2]\n","    elif column == 'fecha_lanzamiento':\n","      if len(dataframe[dataframe[column].isnull()]) == 0:\n","        NormDataframe[column] = (m[0] / m[1])\n","      else:\n","        NormDataframe[column] = (dataframe[column].astype(np.double) - m[0]) / m[1]\n","    elif column == 'reproduccion': \n","      1==1\n","    else:\n","      NormDataframe[column] = (dataframe[column].astype(np.double) - m[0]) / m[1]\n","   \n","  return NormDataframe"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lcg6iba6JyjW"},"source":["import pandas as pd, numpy as np\n","dataframe_part = pd.read_csv('drive/MyDrive/AI/evento_particular.csv')\n","dataframe_part = normalize(dataframe_part, meanStd)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5kcVL7wZLKjS"},"source":["dataframe_part = dataframe_part.drop(['reproduccion'], axis=1)\n","dataframe_part = dataframe_part.drop(['fecha_reproduccion'], axis=1)\n","dataframe_part = dataframe_part.drop(['dia_semana'], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VuCqJBdUTQ_W"},"source":["dataframe_part"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gGopRnfgZ2jz"},"source":["\n","\n","model.eval()\n","Norm_dataframe_part = normalize(dataframe_part, meanStd)\n","\n","X_part = torch.from_numpy(Norm_dataframe_part.to_numpy()).type(torch.FloatTensor)\n","\n","pred = model(X_part)\n","\n","pred"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fHi0I5iSjlKv"},"source":["#Invocación a predicciones y resultados"]}]}